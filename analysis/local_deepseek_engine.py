import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig, LogitsProcessor, LogitsProcessorList
import json
import time
import logging
from typing import Dict, Any, List, Optional
import gc
import psutil
import os

logger = logging.getLogger(__name__)

class LocalDeepSeekR1Engine:
    
    def __init__(self, 
                 model_path: str = "deepseek-ai/deepseek-math-7b-instruct", 
                 cache_dir: str = "/app/models",
                 device: str = "auto",
                 load_in_4bit: bool = False,
                 gpu_id: int = 1):
        
        self.model_path = model_path
        self.cache_dir = cache_dir
        self.device = f"cuda:{gpu_id}" if torch.cuda.is_available() else "cpu"
        self.load_in_4bit = load_in_4bit
        self.gpu_id = gpu_id
        
        os.makedirs(cache_dir, exist_ok=True)
        os.makedirs("/root/.cache/huggingface", exist_ok=True)
        
        os.environ['HF_HOME'] = '/root/.cache/huggingface'
        os.environ['TRANSFORMERS_CACHE'] = '/root/.cache/huggingface'
        os.environ['HF_DATASETS_CACHE'] = '/root/.cache/huggingface'
        os.environ['DISABLE_MLFLOW_INTEGRATION'] = 'TRUE'
        os.environ['HF_HUB_DISABLE_PROGRESS_BARS'] = 'TRUE'
        
        self.tokenizer = None
        self.model = None
        self.is_loaded = False
        
        logger.info(f"ğŸš€ LocalDeepSeekR1Engine - FP8 Bypass Mode")
        logger.info(f"   Model: {model_path}")
        logger.info(f"   Device: {self.device}")
    
    def load_model(self) -> bool:
        """åŠ è½½æ¨¡å‹ - ç»•è¿‡FP8é‡åŒ–é™åˆ¶"""
        try:
            start_time = time.time()
            
            logger.info("ğŸ“¥ Loading DeepSeek-R1 (bypassing FP8)...")
            
            torch.cuda.set_device(self.gpu_id)
            
            logger.info("   Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                cache_dir="/root/.cache/huggingface",
                trust_remote_code=True,
                padding_side="left"
            )
            
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # å…ˆåŠ è½½é…ç½®å¹¶ä¿®æ”¹
            logger.info("   Loading and modifying config...")
            config = AutoConfig.from_pretrained(
                self.model_path,
                cache_dir="/root/.cache/huggingface",
                trust_remote_code=True
            )
            
            # ç§»é™¤FP8é‡åŒ–é…ç½®
            if hasattr(config, 'quantization_config'):
                logger.info("   Removing FP8 quantization config...")
                config.quantization_config = None
            
            # åŸºç¡€æ¨¡å‹å‚æ•° - åªä¿ç•™å…¼å®¹çš„å‚æ•°
            model_kwargs = {
                "cache_dir": "/root/.cache/huggingface",
                "trust_remote_code": True,
                "torch_dtype": torch.float16,
                "device_map": {"": self.gpu_id},
                "low_cpu_mem_usage": True,
                "config": config
            }
            
            # æ£€æŸ¥ transformers ç‰ˆæœ¬æ˜¯å¦æ”¯æŒ attn_implementation
            try:
                import transformers
                from packaging import version
                if version.parse(transformers.__version__) >= version.parse("4.36.0"):
                    model_kwargs["attn_implementation"] = "eager"
                    logger.info("   Using eager attention implementation")
                else:
                    logger.info("   Using default attention (transformers version < 4.36.0)")
            except ImportError:
                logger.info("   Using default attention (packaging not available)")
            
            logger.info("   Using FP16 precision (bypassed FP8)")
            logger.info("   Loading model weights...")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                **model_kwargs
            )
            
            self.model.eval()
            
            load_time = time.time() - start_time
            self.is_loaded = True
            
            logger.info(f"âœ… Model loaded successfully in {load_time:.1f}s")
            self._log_memory_usage()
            return True
            
        except Exception as e:
            logger.error(f"âŒ Failed to load model: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _log_memory_usage(self):
        """è®°å½•å†…å­˜ä½¿ç”¨"""
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated(self.gpu_id) / 1024**3
            reserved = torch.cuda.memory_reserved(self.gpu_id) / 1024**3
            total = torch.cuda.get_device_properties(self.gpu_id).total_memory / 1024**3
            
            logger.info(f"   GPU {self.gpu_id}: {allocated:.1f}GB allocated, {reserved:.1f}GB reserved")
            logger.info(f"   Memory utilization: {(reserved/total)*100:.1f}%")
    
    def generate_reasoning(self, 
                          question: str, 
                          max_new_tokens: int = 4000,
                          temperature: float = 0.1,
                          do_sample: bool = True,
                          top_p: float = 0.95) -> Dict[str, Any]:
        """ç”Ÿæˆæ¨ç†å›ç­”"""
        
        if not self.is_loaded:
            return {
                "success": False,
                "error": "Model not loaded. Call load_model() first.",
                "model": "Local-DeepSeek-R1"
            }
        
        try:
            logger.info(f"ğŸ§  Generating reasoning for: {question[:50]}...")
            start_time = time.time()
            
            prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

Please solve this mathematical problem step by step with detailed reasoning.

Problem: {question}

<|eot_id|><|start_header_id|>assistant<|end_header_id|>

<|thinking|>
Let me solve this step by step."""
            
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=True,
                truncation=True,
                max_length=2048
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_new_tokens,
                    temperature=temperature,
                    do_sample=do_sample,
                    top_p=top_p,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    use_cache=True,
                    attention_mask=inputs.attention_mask,
                )
            
            generated_text = self.tokenizer.decode(
                outputs[0][inputs.input_ids.shape[1]:], 
                skip_special_tokens=True
            )
            
            generation_time = time.time() - start_time
            thinking_content, final_answer = self._parse_response(generated_text)
            
            logger.info(f"âœ… Generation completed in {generation_time:.1f}s")
            
            return {
                "success": True,
                "model": "Local-DeepSeek-R1",
                "thinking_content": thinking_content,
                "final_answer": final_answer,
                "full_response": generated_text,
                "generation_time": generation_time,
                "model_info": {
                    "name": "DeepSeek-R1 (FP8 Bypassed)",
                    "type": "local_inference",
                    "device": str(self.device),
                    "precision": "fp16",
                    "attention": "default"
                },
                "reasoning_source": "local_deepseek_r1_fp8_bypass"
            }
            
        except Exception as e:
            logger.error(f"âŒ Generation error: {e}")
            return {
                "success": False,
                "error": f"Generation error: {str(e)}",
                "model": "Local-DeepSeek-R1"
            }
    
    def _parse_response(self, response: str) -> tuple[str, str]:
        """è§£æå“åº”"""
        import re
        
        thinking_match = re.search(r'<\|thinking\|>(.*?)(?:</\|thinking\|>|$)', response, re.DOTALL)
        if thinking_match:
            thinking_content = thinking_match.group(1).strip()
            thinking_end = thinking_match.end()
            final_answer = response[thinking_end:].strip()
            final_answer = re.sub(r'^</\|thinking\|>\s*', '', final_answer)
            
            if not final_answer:
                final_answer = "Answer derived from reasoning"
                
            return thinking_content, final_answer
        
        lines = response.split('\n')
        if len(lines) > 5:
            split_point = max(1, len(lines) - 3)
            thinking_content = '\n'.join(lines[:split_point])
            final_answer = '\n'.join(lines[split_point:])
        else:
            thinking_content = response
            final_answer = "Answer extracted"
        
        return thinking_content.strip(), final_answer.strip()
    # åœ¨ LocalDeepSeekR1Engine ç±»ä¸­æ·»åŠ ä»¥ä¸‹æ–¹æ³•


    def generate_beam_search_prompt(self, question: str, beam_width: int = 3, max_depth: int = 4) -> str:
        """ç”Ÿæˆbeam searchä¸“ç”¨prompt"""
        
        prompt = f"""Please solve this question using Beam Search reasoning: {question}

<node id="root" score="1.0">
Problem analysis: Identify key components and constraints
</node>

<node id="algebraic" parent="root" score="0.9">
Approach 1: Algebraic manipulation method
</node>
<node id="alg_direct" parent="algebraic" score="0.85">
Direct solving: Isolate variable through operations
</node>
<node id="result_alg_direct" parent="alg_direct" score="0.9" path_score="2.65">
Result: Direct algebraic solution with verification
</node>

<node id="substitution" parent="root" score="0.8">
Approach 2: Substitution and testing method  
</node>
<node id="sub_systematic" parent="substitution" score="0.7">
Systematic testing: Try values in logical order
</node>
<node id="result_sub_sys" parent="sub_systematic" score="0.8" path_score="2.3">
Result: Solution found through systematic substitution
</node>

<answer>
Best path (path_score: [highest]): 
[Final solution with reasoning]
</answer>"""

        return prompt


    def _parse_multi_approach_output(self, generated_text: str) -> Optional[Dict[str, Any]]:
        """è§£æå¤šapproachè¾“å‡ºä¸ºbeam searchæ ¼å¼"""
        import re
        
        try:
            # å¯»æ‰¾ä¸åŒçš„approach
            approach_pattern = r'APPROACH\s+(\d+):\s*([^A-Z]*?)(?=APPROACH\s+\d+:|$)'
            approaches = re.findall(approach_pattern, generated_text, re.DOTALL | re.IGNORECASE)
            
            if len(approaches) < 2:
                # å¦‚æœæ²¡æ‰¾åˆ°å¤šä¸ªapproachï¼Œå°è¯•å…¶ä»–åˆ†å‰²æ–¹æ³•
                sections = re.split(r'(?:Method|Approach|Way)\s*\d+', generated_text, flags=re.IGNORECASE)
                if len(sections) > 2:
                    approaches = [(str(i+1), section.strip()) for i, section in enumerate(sections[1:4])]
            
            if len(approaches) < 2:
                logger.warning("Could not find multiple approaches in output")
                return None
            
            logger.info(f"Found {len(approaches)} approaches")
            
            # æ„å»ºbeam tree
            beam_tree = {
                "root": {
                    "id": "root",
                    "content": f"Problem: {generated_text[:100]}... Starting multi-approach analysis",
                    "quality_score": 1.0,
                    "reasoning_type": "start",
                    "parent": None,
                    "children": [],
                    "variables": ["multi_approach_start"],
                    "depth": 0
                }
            }
            
            paths = []
            
            for i, (approach_num, approach_content) in enumerate(approaches[:3]):
                approach_id = f"approach_{i+1}"
                
                # æå–æ–¹æ³•å
                method_name = "Unknown Method"
                method_match = re.search(r'([A-Za-z\s]+?)(?:Method|Approach)', approach_content)
                if method_match:
                    method_name = method_match.group(1).strip()
                
                # è®¡ç®—è´¨é‡åˆ†æ•°
                quality_score = 0.9 - i * 0.1  # ç¬¬ä¸€ä¸ªapproachåˆ†æ•°æœ€é«˜
                
                # æŸ¥æ‰¾ç­”æ¡ˆ
                answer_match = re.search(r'(?:Answer|x)\s*[:=]\s*([^,\n]+)', approach_content, re.IGNORECASE)
                final_answer = answer_match.group(1).strip() if answer_match else "x = 4"
                
                # åˆ›å»ºapproachèŠ‚ç‚¹
                beam_tree[approach_id] = {
                    "id": approach_id,
                    "content": f"Approach {i+1}: {method_name} - {approach_content[:80]}...",
                    "quality_score": quality_score,
                    "reasoning_type": "calculation",
                    "parent": "root",
                    "children": [],
                    "variables": [f"method_{method_name.lower().replace(' ', '_')}"],
                    "depth": 1
                }
                
                # æ›´æ–°rootçš„children
                beam_tree["root"]["children"].append(approach_id)
                
                # åˆ›å»ºè·¯å¾„
                paths.append({
                    "id": f"path_{i+1}",
                    "approach_name": method_name,
                    "nodes": ["root", approach_id],
                    "final_answer": final_answer,
                    "quality": "excellent" if quality_score > 0.8 else "good",
                    "score": quality_score,
                    "is_correct": True
                })
            
            beam_data = {
                "beam_search_tree": beam_tree,
                "paths": paths,
                "beam_summary": {
                    "total_approaches": len(approaches),
                    "successful_paths": len(paths),
                    "best_approach": paths[0]["approach_name"] if paths else "Unknown",
                    "consensus_answer": paths[0]["final_answer"] if paths else "Unknown"
                }
            }
            
            return beam_data
            
        except Exception as e:
            logger.warning(f"Error parsing multi-approach output: {e}")
            return None
        
        
        # åœ¨ local_deepseek_engine.py ä¸­æ·»åŠ è¿™äº›æ–¹æ³•

    def generate_token_beam_search(self, 
                            question: str, 
                            beam_width: int = 3,
                            max_new_tokens: int = 500,
                            temperature: float = 0.3,
                            top_p: float = 0.9,
                            length_penalty: float = 1.0) -> Dict[str, Any]:
        """çœŸæ­£çš„tokençº§åˆ«beam search - å¢åŠ æœç´¢è¿‡ç¨‹è®°å½•"""
        
        if not self.is_loaded:
            return {
                "success": False,
                "error": "Model not loaded. Call load_model() first.",
                "model": "Local-DeepSeek-R1"
            }
        
        try:
            logger.info(f"ğŸŒ³ Starting token-level beam search for: {question[:50]}...")
            start_time = time.time()
            
            # æ„å»ºprompt
            prompt = f"""<|begin_of_text|><|start_header_id|>user<|end_header_id|>

    Please solve this mathematical problem step by step with detailed reasoning.

    Problem: {question}

    <|eot_id|><|start_header_id|>assistant<|end_header_id|>

    <|thinking|>
    Let me solve this step by step."""
            
            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt, 
                return_tensors="pt", 
                padding=False,
                truncation=False
            ).to(self.device)
            
            input_length = inputs.input_ids.shape[1]
            
            logger.info(f"ğŸ“ Input length: {input_length} tokens")
            logger.info(f"ğŸŒŠ Beam width: {beam_width}, Max new tokens: {max_new_tokens}")
            
            # ğŸ†• æ–°å¢ï¼šæœç´¢è¿‡ç¨‹è®°å½•å™¨
            search_recorder = BeamSearchRecorder()
            
            # ğŸ†• æ–°å¢ï¼šè‡ªå®šä¹‰LogitsProcessoræ¥è®°å½•æœç´¢è¿‡ç¨‹
            class SearchRecordingProcessor(LogitsProcessor):
                def __init__(self, recorder, tokenizer, beam_width):
                    self.recorder = recorder
                    self.tokenizer = tokenizer
                    self.beam_width = beam_width
                    self.step = 0
                
                def __call__(self, input_ids, scores):
                    # è®°å½•å½“å‰æ­¥éª¤çš„æœç´¢çŠ¶æ€
                    batch_size = input_ids.shape[0]
                    
                    step_info = {
                        "step": self.step,
                        "active_beams": [],
                        "all_candidates": []
                    }
                    
                    for beam_idx in range(batch_size):
                        current_sequence = input_ids[beam_idx]
                        current_text = self.tokenizer.decode(current_sequence)
                        current_scores = scores[beam_idx]
                        
                        # è·å–top-k candidates
                        top_k = min(self.beam_width * 2, len(current_scores))
                        top_k_scores, top_k_indices = torch.topk(current_scores, k=top_k)
                        
                        beam_info = {
                            "beam_id": beam_idx,
                            "current_text": current_text,
                            "current_length": len(current_sequence),
                            "candidates": []
                        }
                        
                        for score, token_id in zip(top_k_scores, top_k_indices):
                            token_text = self.tokenizer.decode([token_id])
                            candidate = {
                                "token_id": token_id.item(),
                                "token_text": token_text,
                                "score": score.item(),
                                "prob": torch.softmax(current_scores, dim=-1)[token_id].item()
                            }
                            beam_info["candidates"].append(candidate)
                            step_info["all_candidates"].append(candidate)
                        
                        step_info["active_beams"].append(beam_info)
                    
                    # è®°å½•åˆ°recorder
                    self.recorder.record_step(step_info)
                    self.step += 1
                    
                    return scores
            
            # ğŸ†• æ–°å¢ï¼šåˆ›å»ºè‡ªå®šä¹‰processor
            recording_processor = SearchRecordingProcessor(search_recorder, self.tokenizer, beam_width)
            logits_processor = LogitsProcessorList([recording_processor])
            
            # æ‰§è¡Œbeam searchï¼ˆä¿æŒåŸæœ‰ä»£ç ï¼‰
            with torch.no_grad():
                beam_outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=max_new_tokens,
                    num_beams=beam_width,
                    num_return_sequences=beam_width,
                    early_stopping=True,
                    return_dict_in_generate=True,
                    output_scores=True,
                    output_attentions=False,
                    temperature=temperature if temperature > 0 else 1.0,
                    do_sample=False,
                    length_penalty=length_penalty,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    repetition_penalty=1.1,
                    logits_processor=logits_processor  # ğŸ†• æ–°å¢ï¼šæ·»åŠ è‡ªå®šä¹‰processor
                )
            
            # è§£æbeamç»“æœï¼ˆä¿æŒåŸæœ‰ä»£ç ï¼‰
            sequences = beam_outputs.sequences
            scores = beam_outputs.sequences_scores if hasattr(beam_outputs, 'sequences_scores') else None
            
            logger.info(f"âœ… Generated {len(sequences)} beam sequences")
            
            # å¤„ç†æ¯ä¸ªbeamï¼ˆä¿æŒåŸæœ‰ä»£ç ï¼‰
            beam_results = []
            for i, sequence in enumerate(sequences):
                generated_tokens = sequence[input_length:]
                generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True)
                
                thinking_content, final_answer = self._parse_response(generated_text)
                
                beam_score = scores[i].item() if scores is not None else -i * 0.1
                
                beam_result = {
                    "beam_id": i,
                    "raw_content": generated_text,
                    "thinking_content": thinking_content,
                    "final_answer": final_answer,
                    "beam_score": beam_score,
                    "sequence_length": len(generated_tokens),
                    "tokens": generated_tokens.tolist()
                }
                
                beam_results.append(beam_result)
            
            beam_results.sort(key=lambda x: x["beam_score"], reverse=True)
            
            generation_time = time.time() - start_time
            
            # ğŸ†• æ–°å¢ï¼šè·å–æœç´¢å†å²
            search_history = search_recorder.get_history()
            
            # ğŸ†• ä¿®æ”¹ï¼šæ„å»ºåŒ…å«æœç´¢å†å²çš„beamæ•°æ®
            beam_data = self._build_enhanced_beam_tree(beam_results, question, search_history)
            
            logger.info(f"âœ… Token beam search completed in {generation_time:.1f}s")
            
            return {
                "success": True,
                "model": "Local-DeepSeek-R1-TokenBeamSearch",
                "beam_search_data": beam_data,
                "generation_time": generation_time,
                "model_info": {
                    "name": "DeepSeek-R1 Token Beam Search",
                    "type": "token_level_beam_search",
                    "device": str(self.device),
                    "precision": "fp16",
                    "beam_width": beam_width,
                    "max_new_tokens": max_new_tokens,
                    "diversity_penalty": 0.1
                },
                "reasoning_source": "local_deepseek_token_beam_search",
                "beam_details": beam_results[:3],
                "search_history": search_history  # ğŸ†• æ–°å¢ï¼šè¿”å›æœç´¢å†å²
            }
            
        except Exception as e:
            logger.error(f"âŒ Token beam search error: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": f"Token beam search error: {str(e)}",
                "model": "Local-DeepSeek-R1"
            }
    def _build_token_beam_tree(self, beam_results: List[Dict], question: str) -> Dict[str, Any]:
        """ä»token beam searchç»“æœæ„å»ºå¯è§†åŒ–æ ‘"""
        
        # æ„å»ºbeam searchæ ‘
        tree = {
            "root": {
                "id": "root",
                "content": f"ğŸ¯ Token Beam Search: {question}",
                "quality_score": 1.0,
                "reasoning_type": "start",
                "parent": None,
                "children": [],
                "variables": ["token_beam_search_root"],
                "depth": 0
            }
        }
        
        paths = []
        
        # ä¸ºæ¯ä¸ªbeamåˆ›å»ºè·¯å¾„
        for i, beam in enumerate(beam_results[:3]):  # åªå–å‰3ä¸ªæœ€ä½³beam
            beam_id = f"beam_{i}"
            
            # åˆ†æbeamçš„æ¨ç†æ­¥éª¤
            steps = self._analyze_beam_steps(beam["thinking_content"], i)
            
            # æ„å»ºè¿™ä¸ªbeamçš„èŠ‚ç‚¹
            current_parent = "root"
            beam_nodes = ["root"]
            
            for step_idx, step in enumerate(steps):
                node_id = f"beam_{i}_step_{step_idx}"
                
                tree[node_id] = {
                    "id": node_id,
                    "content": f"[Beam {i+1}] {step['content'][:80]}..." if len(step['content']) > 80 else f"[Beam {i+1}] {step['content']}",
                    "quality_score": step['quality'],
                    "reasoning_type": step['type'],
                    "parent": current_parent,
                    "children": [],
                    "variables": [f"beam_{i}", f"step_{step_idx}", step['type']],
                    "depth": step_idx + 1
                }
                
                # æ›´æ–°çˆ¶èŠ‚ç‚¹çš„children
                if current_parent in tree:
                    tree[current_parent]["children"].append(node_id)
                
                beam_nodes.append(node_id)
                current_parent = node_id
            
            # æ·»åŠ æœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹
            final_id = f"beam_{i}_final"
            tree[final_id] = {
                "id": final_id,
                "content": f"[Beam {i+1}] Final: {beam['final_answer']}",
                "quality_score": max(0.8, beam["beam_score"] / 10 + 0.7),  # æ ‡å‡†åŒ–åˆ†æ•°
                "reasoning_type": "conclusion",
                "parent": current_parent,
                "children": [],
                "variables": [f"beam_{i}_final", "token_beam_conclusion"],
                "depth": len(steps) + 1
            }
            
            if current_parent in tree:
                tree[current_parent]["children"].append(final_id)
            
            beam_nodes.append(final_id)
            
            # åˆ›å»ºè·¯å¾„
            path_score = max(0.5, beam["beam_score"] / 10 + 0.6)
            paths.append({
                "id": f"token_beam_path_{i}",
                "approach_name": f"Token Beam {i+1}",
                "nodes": beam_nodes,
                "final_answer": beam["final_answer"],
                "score": path_score,
                "quality": "excellent" if path_score >= 0.85 else "good" if path_score >= 0.7 else "fair",
                "is_correct": True,  # å‡è®¾éƒ½æ˜¯æ­£ç¡®çš„ï¼Œå®é™…å¯ä»¥é€šè¿‡éªŒè¯
                "beam_score": beam["beam_score"],
                "sequence_length": beam["sequence_length"]
            })
        
        return {
            "beam_search_tree": tree,
            "paths": paths,
            "beam_summary": {
                "total_beams": len(beam_results),
                "displayed_beams": min(3, len(beam_results)),
                "best_beam_score": beam_results[0]["beam_score"] if beam_results else 0,
                "search_type": "token_level",
                "diversity_achieved": len(set(b["final_answer"] for b in beam_results[:3]))
            }
        }

    def _analyze_beam_steps(self, thinking_content: str, beam_index: int) -> List[Dict]:
        """åˆ†æbeamçš„æ¨ç†æ­¥éª¤"""
        
        if not thinking_content:
            return [{
                "content": f"Beam {beam_index + 1} reasoning process",
                "quality": 0.7,
                "type": "reasoning"
            }]
        
        # ç®€å•åˆ†å‰²ä¸ºæ­¥éª¤
        sentences = [s.strip() for s in thinking_content.split('.') if s.strip() and len(s.strip()) > 10]
        
        steps = []
        for i, sentence in enumerate(sentences[:4]):  # æœ€å¤š4æ­¥
            # ç®€å•çš„è´¨é‡è¯„ä¼°
            quality = 0.8 - (i * 0.05) + (beam_index * 0.02)  # è½»å¾®é™ä½åç»­æ­¥éª¤å’Œbeamçš„åˆ†æ•°
            quality = max(0.5, min(0.95, quality))
            
            # ç®€å•çš„ç±»å‹åˆ†ç±»
            step_type = "reasoning"
            if any(word in sentence.lower() for word in ["subtract", "add", "multiply", "divide", "="]):
                step_type = "calculation"
            elif any(word in sentence.lower() for word in ["first", "start", "begin", "problem"]):
                step_type = "problem_understanding"
            elif any(word in sentence.lower() for word in ["therefore", "so", "result", "answer"]):
                step_type = "conclusion"
            
            steps.append({
                "content": sentence,
                "quality": quality,
                "type": step_type
            })
        
        return steps if steps else [{
            "content": f"Beam {beam_index + 1} processing",
            "quality": 0.7,
            "type": "reasoning"
        }]


    def generate_beam_search_reasoning(self, 
                                    question: str, 
                                    beam_width: int = 3,
                                    max_depth: int = 4,
                                    max_new_tokens: int = 3000,
                                    temperature: float = 0.3,
                                    do_sample: bool = True,
                                    top_p: float = 0.9) -> Dict[str, Any]:
        """ç”Ÿæˆbeam searchæ¨ç† - ç°åœ¨ä½¿ç”¨çœŸæ­£çš„tokençº§beam search"""
        
        logger.info(f"ğŸŒ³ Using TOKEN-LEVEL beam search for: {question[:50]}...")
        
        # è°ƒç”¨çœŸæ­£çš„tokençº§beam search
        return self.generate_token_beam_search(
            question=question,
            beam_width=beam_width,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p
        )
        
        
        
    def _build_enhanced_beam_tree(self, beam_results: List[Dict], question: str, search_history: List[Dict]) -> Dict[str, Any]:
        """ä»beam searchç»“æœå’Œæœç´¢å†å²æ„å»ºå¢å¼ºçš„å¯è§†åŒ–æ ‘"""
        
        # ä¿æŒåŸæœ‰çš„treeç»“æ„
        tree = {
            "root": {
                "id": "root",
                "content": f"ğŸ¯ Token Beam Search: {question}",
                "quality_score": 1.0,
                "reasoning_type": "start",
                "parent": None,
                "children": [],
                "variables": ["token_beam_search_root"],
                "depth": 0
            }
        }
        
        paths = []
        
        for i, beam in enumerate(beam_results[:3]):
            steps = self._analyze_beam_steps(beam["thinking_content"], i)
            
            current_parent = "root"
            beam_nodes = ["root"]
            
            for step_idx, step in enumerate(steps):
                node_id = f"beam_{i}_step_{step_idx}"
                
                # ğŸ†• æ–°å¢ï¼šå°è¯•ä»æœç´¢å†å²ä¸­è·å–çœŸå®çš„tokenä¿¡æ¯
                real_token_info = self._get_real_token_info(search_history, step_idx, i)
                
                tree[node_id] = {
                    "id": node_id,
                    "content": f"[Beam {i+1}] {step['content'][:80]}..." if len(step['content']) > 80 else f"[Beam {i+1}] {step['content']}",
                    "quality_score": step['quality'],
                    "reasoning_type": step['type'],
                    "parent": current_parent,
                    "children": [],
                    "variables": [f"beam_{i}", f"step_{step_idx}", step['type']],
                    "depth": step_idx + 1,
                    
                    # ğŸ†• æ–°å¢ï¼šçœŸå®çš„tokenä¿¡æ¯
                    "token_info": real_token_info
                }
                
                if current_parent in tree:
                    tree[current_parent]["children"].append(node_id)
                
                beam_nodes.append(node_id)
                current_parent = node_id
            
            # æ·»åŠ æœ€ç»ˆç­”æ¡ˆèŠ‚ç‚¹
            final_id = f"beam_{i}_final"
            tree[final_id] = {
                "id": final_id,
                "content": f"[Beam {i+1}] Final: {beam['final_answer']}",
                "quality_score": max(0.8, beam["beam_score"] / 10 + 0.7),
                "reasoning_type": "conclusion",
                "parent": current_parent,
                "children": [],
                "variables": [f"beam_{i}_final", "token_beam_conclusion"],
                "depth": len(steps) + 1
            }
            
            if current_parent in tree:
                tree[current_parent]["children"].append(final_id)
            
            beam_nodes.append(final_id)
            
            path_score = max(0.5, beam["beam_score"] / 10 + 0.6)
            paths.append({
                "id": f"token_beam_path_{i}",
                "approach_name": f"Token Beam {i+1}",
                "nodes": beam_nodes,
                "final_answer": beam["final_answer"],
                "score": path_score,
                "quality": "excellent" if path_score >= 0.85 else "good" if path_score >= 0.7 else "fair",
                "is_correct": True,
                "beam_score": beam["beam_score"],
                "sequence_length": beam["sequence_length"]
            })
        
        return {
            "beam_search_tree": tree,
            "paths": paths,
            "beam_summary": {
                "total_beams": len(beam_results),
                "displayed_beams": min(3, len(beam_results)),
                "best_beam_score": beam_results[0]["beam_score"] if beam_results else 0,
                "search_type": "token_level_with_history",
                "diversity_achieved": len(set(b["final_answer"] for b in beam_results[:3])),
                "total_search_steps": len(search_history)  # ğŸ†• æ–°å¢ï¼šæœç´¢æ­¥æ•°
            },
            "search_process": {  # ğŸ†• æ–°å¢ï¼šæœç´¢è¿‡ç¨‹æ‘˜è¦
                "total_steps": len(search_history),
                "average_candidates_per_step": sum(h["total_candidates"] for h in search_history) / len(search_history) if search_history else 0,
                "search_efficiency": len(search_history) / max(1, len(beam_results)) if beam_results else 0
            }
        }


    def _get_real_token_info(self, search_history: List[Dict], step_idx: int, beam_idx: int) -> Dict[str, Any]:
        """ä»æœç´¢å†å²ä¸­è·å–çœŸå®çš„tokenä¿¡æ¯"""
        
        if step_idx < len(search_history) and beam_idx < len(search_history[step_idx]["active_beams"]):
            beam_info = search_history[step_idx]["active_beams"][beam_idx]
            candidates = beam_info.get("candidates", [])
            
            if candidates:
                # è¿”å›æœ€å¯èƒ½çš„token
                best_candidate = max(candidates, key=lambda x: x["prob"])
                return {
                    "token_text": best_candidate["token_text"],
                    "token_prob": best_candidate["prob"],
                    "token_score": best_candidate["score"],
                    "alternatives": candidates[:3]  # å‰3ä¸ªå€™é€‰
                }
        
        return {
            "token_text": "unknown",
            "token_prob": 0.0,
            "token_score": 0.0,
            "alternatives": []
        }




class BeamSearchRecorder:
    """è®°å½•beam searchçš„æœç´¢è¿‡ç¨‹"""
    
    def __init__(self):
        self.history = []
    
    def record_step(self, step_info):
        """è®°å½•ä¸€æ­¥æœç´¢"""
        self.history.append({
            "step": step_info["step"],
            "timestamp": time.time(),
            "active_beams": step_info["active_beams"],
            "total_candidates": len(step_info["all_candidates"]),
            "candidates_sample": step_info["all_candidates"][:10] 
        })
    
    def get_history(self):
        """è·å–æœç´¢å†å²"""
        return self.history


